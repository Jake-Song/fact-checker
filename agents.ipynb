{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict, Any, Optional, Callable\n",
    "from abc import ABC, abstractmethod\n",
    "import json\n",
    "from dataclasses import dataclass\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "@dataclass\n",
    "class Message:\n",
    "    role: str\n",
    "    content: str\n",
    "    metadata: Optional[Dict[str, Any]] = None\n",
    "\n",
    "class LLMProvider(ABC):\n",
    "    @abstractmethod\n",
    "    def generate(self, prompt: str, **kwargs) -> str:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OpenAIProvider(LLMProvider):\n",
    "    def __init__(self, model: str = \"gpt-4o-mini\"):\n",
    "        from openai import OpenAI\n",
    "        self.client = OpenAI()\n",
    "        self.model = model\n",
    "\n",
    "    def generate(self, prompt: List[Message], **kwargs) -> tuple[str | dict, bool]:\n",
    "        response = self.client.chat.completions.create(\n",
    "            model=self.model,\n",
    "            messages=prompt,\n",
    "            **kwargs\n",
    "        )\n",
    "        if response.choices[0].message.tool_calls is None:\n",
    "            return response.choices[0].message.content, False\n",
    "        else:\n",
    "            return response.choices[0].message, True\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAIProvider()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tavily import TavilyClient\n",
    "tavily_client = TavilyClient(api_key=f\"{os.getenv('TAVILY_API_KEY')}\")\n",
    "\n",
    "def search_tavily(query):\n",
    "    response = tavily_client.search(\n",
    "        query=query\n",
    "    )\n",
    "    return response['results']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [{\n",
    "    \"type\": \"function\",\n",
    "    \"function\": {\n",
    "        \"name\": \"search_tavily\",\n",
    "        \"description\": \"Search the web for a given query.\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"query\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"query to search the web for\"\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\n",
    "                \"query\"\n",
    "            ],\n",
    "            \"additionalProperties\": False\n",
    "        },\n",
    "        \"strict\": True\n",
    "    }\n",
    "}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatCompletionMessage(content=None, refusal=None, role='assistant', audio=None, function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='call_iUrHCbMUlIRLuVByAIvY1riv', function=Function(arguments='{\"query\":\"climate crisis\"}', name='search_tavily'), type='function')])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.generate(\"search on 'climate crisis'\", tools=tools) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tool:\n",
    "    def __init__(\n",
    "        self,\n",
    "        description: List[Dict[str, Any]],\n",
    "        function: Callable\n",
    "    ):\n",
    "        self.description = description\n",
    "        self.function = function\n",
    "   \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(\n",
    "        self,\n",
    "        llm: LLMProvider,\n",
    "        system_prompt: str,\n",
    "        tool: Tool = None\n",
    "    ):\n",
    "        self.llm = llm\n",
    "        self.system_prompt = system_prompt\n",
    "        self.messages = [{\"role\": \"system\", \"content\": self.system_prompt}]\n",
    "        self.tool = tool\n",
    "\n",
    "    def generate(self, prompt: str, **kwargs) -> str:\n",
    "        self.messages.append({\"role\": \"user\", \"content\": prompt})\n",
    "\n",
    "        if self.tool:\n",
    "            completion, is_tool_call = self.llm.generate(\n",
    "                prompt=self.messages,\n",
    "                tools=self.tool.description\n",
    "            )\n",
    "            \n",
    "            if is_tool_call:\n",
    "                tool_call = completion.tool_calls[0]\n",
    "                args = json.loads(tool_call.function.arguments)\n",
    "\n",
    "                result = self.tool.function(args[\"query\"])\n",
    "                self.messages.append(completion)\n",
    "                self.messages.append({                             \n",
    "                    \"role\": \"tool\",\n",
    "                    \"tool_call_id\": tool_call.id,\n",
    "                    \"content\": str(result)\n",
    "                })\n",
    "                \n",
    "                completion_2, _ = self.llm.generate(\n",
    "                    prompt=self.messages,\n",
    "                )\n",
    "\n",
    "                return completion_2\n",
    "            else:\n",
    "                return completion\n",
    "        else:\n",
    "            completion = self.llm.generate(\n",
    "                prompt=self.messages                \n",
    "            )\n",
    "            return completion\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_tool = Tool(tools, search_tavily)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_agent = Agent(\n",
    "    llm=llm, \n",
    "    system_prompt=\"사용자가 요청한 내용에 대한 팩트체크를 위한 인터넷 검색을 해주는 도우미입니다.\",\n",
    "    tool=search_tool\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "fact_check_agent = Agent(\n",
    "    llm=llm, \n",
    "    system_prompt=\"주어진 내용을 요약하고 팩트체크를 해주는 도우미입니다.\"    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "searched_result = search_agent.generate(\"지구는 평평하다\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'지구가 평평하다는 주장은 과학적으로 근거가 없습니다. 현대 과학은 지구가 둥글다는 것을 명확한 증거로 입증하였습니다. 평평한 지구론자들은 여러 가지 주장과 이론을 제기하지만, 이들은 주로 음모론적 성격을 띠고 있으며 과학적 사실과 반대되는 내용입니다.\\n\\n예를 들어, 지구의 곡률, 위성 사진, 항공기 비행 경로와 같은 여러 과학적 데이터는 지구가 둥글다는 것을 지지합니다. NASA의 이미지와 우주 탐사기술 등도 지구의 구형을 나타냅니다. 평면 지구론은 현대 과학의 이해와 부합하지 않으며, 이를 믿는 사람들은 종종 잘못된 정보와 음모론에 기반해 설명을 시도합니다.\\n\\n더 많은 정보는 [여기](https://blog.naver.com/PostView.naver?blogId=bab_bab_2&logNo=223729348974)에서 확인할 수 있습니다.'"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "searched_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('주어진 내용은 지구가 평평하다는 주장에 대한 반론입니다. 요약하자면, 현대 과학은 지구가 둥글다는 것을 여러 가지 증거를 통해 입증하고 있으며, 평평한 지구론은 과학적 사실과 반대되는 음모론적 주장을 포함하고 있다고 설명합니다. 증거로는 지구의 곡률, 위성 사진, 항공기 비행 경로 등을 들고 있습니다.\\n\\n이와 관련하여 사실 확인을 해보면, 지구가 둥글다는 것은 과학적 합의로, 많은 실증적 증거에 의해 지지되고 있습니다. NASA의 이미지나 기타 우주 탐사 데이터는 지구의 구형성을 뒷받침하며, 평면 지구론은 현대 과학의 이해와 명백히 상반되는 주장입니다. 따라서, 평면 지구론이 과학적 근거가 없다는 주장은 사실입니다.',\n",
       " False)"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fact_check_agent.generate(searched_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
